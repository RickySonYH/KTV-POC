// [advice from AI] ë¹„ë””ì˜¤ ìš”ì†Œì—ì„œ ì§ì ‘ ì˜¤ë””ì˜¤ ì¶”ì¶œ + WhisperLiveKit ì‹¤ì‹œê°„ STT
// getDisplayMedia ë¶ˆí•„ìš” - video.captureStream() ì‚¬ìš©

import { useState, useRef, useCallback, useEffect } from 'react';
// [advice from AI] í›„ì²˜ë¦¬ëŠ” App.tsxì—ì„œ ë¬¸ì¥ë³„ë¡œ ì ìš© (ì—¬ê¸°ì„œëŠ” ì›ë³¸ ì „ë‹¬)

export interface VideoAudioSubtitle {
  id: number;
  text: string;
  speaker?: string;
  startTime: number;
  endTime: number;
  isFinal: boolean;
}

export interface BufferUpdate {
  text: string;
  speaker?: string;
  isNoAudio?: boolean;  // [advice from AI] ì˜¤ë””ì˜¤ ì—†ìŒ/ìŒì•… ê°ì§€ìš©
}

interface UseVideoAudioSTTProps {
  getVideoElement: () => HTMLVideoElement | null;  // [advice from AI] í•¨ìˆ˜ë¡œ ë°›ì•„ì„œ ìœ ì—°í•˜ê²Œ
  onSubtitle: (subtitle: VideoAudioSubtitle) => void;
  onBufferUpdate?: (buffer: BufferUpdate) => void;
  onStatusChange?: (status: 'idle' | 'connecting' | 'capturing' | 'error') => void;
  wsUrl?: string;
}

// [advice from AI] ë™ì  WebSocket URL ìƒì„±
const getWsUrl = () => {
  if (window.location.protocol === 'https:') {
    return `wss://${window.location.host}/asr`;
  }
  if (window.location.hostname === 'localhost' || window.location.hostname === '127.0.0.1') {
    return 'ws://localhost:6470/asr';
  }
  return `ws://${window.location.hostname}:6470/asr`;
};

// [advice from AI] WhisperLiveKit ì‹œê°„ ë¬¸ìì—´ íŒŒì‹± ("0:00:05" â†’ 5.0)
const parseTimeString = (timeStr: string | number | undefined): number | null => {
  if (typeof timeStr === 'number') return timeStr;
  if (!timeStr || typeof timeStr !== 'string') return null;
  
  // "H:MM:SS" ë˜ëŠ” "HH:MM:SS" í˜•ì‹
  const parts = timeStr.split(':');
  if (parts.length === 3) {
    const [h, m, s] = parts.map(Number);
    return h * 3600 + m * 60 + s;
  }
  // "MM:SS" í˜•ì‹
  if (parts.length === 2) {
    const [m, s] = parts.map(Number);
    return m * 60 + s;
  }
  return null;
};

export function useVideoAudioSTT({ getVideoElement, onSubtitle, onBufferUpdate, onStatusChange, wsUrl }: UseVideoAudioSTTProps) {
  const [isCapturing, setIsCapturing] = useState(false);
  const [status, setStatus] = useState<'idle' | 'connecting' | 'capturing' | 'error'>('idle');
  
  const wsRef = useRef<WebSocket | null>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const videoElementRef = useRef<HTMLVideoElement | null>(null);  // [advice from AI] í˜„ì¬ ë¹„ë””ì˜¤ ìš”ì†Œ ì €ì¥
  // [advice from AI] ìœ ë‹ˆí¬ ID ìƒì„± - timestamp ê¸°ë°˜ + í° ì˜¤í”„ì…‹ìœ¼ë¡œ App.tsxì™€ ì¶©ëŒ ë°©ì§€
  const segmentIdRef = useRef(Date.now() + 1000000);
  const lastLinesCountRef = useRef(0);
  // [advice from AI] ìº¡ì²˜ ì‹œì‘ ì‹œì ì˜ ë¹„ë””ì˜¤ ì‹œê°„ (íƒ€ì„ìŠ¤íƒ¬í”„ ê³„ì‚°ìš©)
  const captureStartVideoTimeRef = useRef(0);
  const lastSpeakerRef = useRef<number | undefined>(undefined);

  const updateStatus = useCallback((newStatus: 'idle' | 'connecting' | 'capturing' | 'error') => {
    setStatus(newStatus);
    onStatusChange?.(newStatus);
  }, [onStatusChange]);

  // [advice from AI] ë¹„ë””ì˜¤ ì˜¤ë””ì˜¤ ìº¡ì²˜ ì‹œì‘
  const startCapture = useCallback(async () => {
    const video = getVideoElement();
    if (!video) {
      console.error('[VIDEO-STT] âŒ ë¹„ë””ì˜¤ ìš”ì†Œê°€ ì—†ìŠµë‹ˆë‹¤');
      alert('ë¨¼ì € ë¹„ë””ì˜¤ íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”.');
      return;
    }
    
    videoElementRef.current = video;  // ì €ì¥

    try {
      updateStatus('connecting');
      console.log('[VIDEO-STT] ğŸ¤ ë¹„ë””ì˜¤ ì˜¤ë””ì˜¤ ìº¡ì²˜ ì‹œì‘...');
      console.log('[VIDEO-STT] ğŸ“º ë¹„ë””ì˜¤ ì†ŒìŠ¤:', video.src?.substring(0, 80) || video.currentSrc?.substring(0, 80));
      
      // [advice from AI] video.captureStream()ìœ¼ë¡œ MediaStream ì–»ê¸°
      let stream: MediaStream;
      try {
        // @ts-ignore - captureStreamì€ í‘œì¤€ APIì§€ë§Œ íƒ€ì… ì •ì˜ì— ì—†ìŒ
        stream = video.captureStream ? video.captureStream() : video.mozCaptureStream?.();
      } catch (captureError) {
        console.error('[VIDEO-STT] âŒ captureStream ì˜¤ë¥˜:', captureError);
        throw new Error('ë¹„ë””ì˜¤ ìº¡ì²˜ ì‹¤íŒ¨: ì™¸ë¶€ URLì€ CORS ì •ì±…ìœ¼ë¡œ ì¸í•´ ìº¡ì²˜í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ì„ ì§ì ‘ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.');
      }
      
      if (!stream) {
        throw new Error('ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ìº¡ì²˜ë¥¼ ì§€ì›í•˜ì§€ ì•ŠëŠ” ë¸Œë¼ìš°ì €ì…ë‹ˆë‹¤.');
      }

      const audioTracks = stream.getAudioTracks();
      console.log('[VIDEO-STT] ğŸ” ì˜¤ë””ì˜¤ íŠ¸ë™ ìˆ˜:', audioTracks.length);
      
      if (audioTracks.length === 0) {
        // [advice from AI] CORSë¡œ ì¸í•´ ì˜¤ë””ì˜¤ íŠ¸ë™ì´ ë¹„ì–´ìˆì„ ìˆ˜ ìˆìŒ
        console.warn('[VIDEO-STT] âš ï¸ ì˜¤ë””ì˜¤ íŠ¸ë™ ì—†ìŒ - CORS ë˜ëŠ” ë¯¸ë””ì–´ ë¡œë“œ ëŒ€ê¸° ì¤‘');
        throw new Error('ì˜¤ë””ì˜¤ íŠ¸ë™ì´ ì—†ìŠµë‹ˆë‹¤. ë¹„ë””ì˜¤ê°€ ì™„ì „íˆ ë¡œë“œë˜ì§€ ì•Šì•˜ê±°ë‚˜ CORS ë¬¸ì œì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.');
      }

      console.log('[VIDEO-STT] âœ… ì˜¤ë””ì˜¤ íŠ¸ë™:', audioTracks[0].label || 'default');

      // WebSocket ì—°ê²°
      const url = wsUrl || getWsUrl();
      console.log('[VIDEO-STT] ğŸ”Œ WebSocket ì—°ê²°:', url);
      
      const ws = new WebSocket(url);
      wsRef.current = ws;

      ws.onopen = async () => {
        console.log('[VIDEO-STT] âœ… WebSocket ì—°ê²° ì„±ê³µ');
        
        // [advice from AI] AudioContext ìƒì„± - ë¹„ë””ì˜¤ì˜ ì˜¤ë””ì˜¤ë¥¼ ì²˜ë¦¬
        const audioContext = new AudioContext();
        audioContextRef.current = audioContext;
        
        const actualSampleRate = audioContext.sampleRate;
        const targetSampleRate = 16000;
        const resampleRatio = actualSampleRate / targetSampleRate;
        
        console.log(`[VIDEO-STT] ğŸ“¼ AudioContext: ${actualSampleRate}Hz â†’ ${targetSampleRate}Hz`);
        
        // [advice from AI] MediaStreamSource ì‚¬ìš© - CORS ë¬¸ì œ íšŒí”¼ + ì¬ì‚¬ìš© ê°€ëŠ¥
        // captureStream()ì—ì„œ ì–»ì€ ìŠ¤íŠ¸ë¦¼ ì§ì ‘ ì‚¬ìš©
        const source = audioContext.createMediaStreamSource(stream);
        
        // [advice from AI] ë¶„ì„ìš© ë…¸ë“œ - ScriptProcessorë¡œ PCM ì¶”ì¶œ
        const bufferSize = 4096;
        const processor = audioContext.createScriptProcessor(bufferSize, 1, 1);
        
        let chunkCount = 0;
        
        // ë‹¤ìš´ìƒ˜í”Œë§ í•¨ìˆ˜
        const downsample = (inputData: Float32Array, ratio: number): Float32Array => {
          if (ratio === 1) return inputData;
          const outputLength = Math.floor(inputData.length / ratio);
          const output = new Float32Array(outputLength);
          for (let i = 0; i < outputLength; i++) {
            output[i] = inputData[Math.floor(i * ratio)];
          }
          return output;
        };
        
        processor.onaudioprocess = (e) => {
          if (ws.readyState !== WebSocket.OPEN) return;
          
          // [advice from AI] ë¹„ë””ì˜¤ê°€ ì¬ìƒ ì¤‘ì¼ ë•Œë§Œ ì „ì†¡
          if (video.paused || video.ended) return;
          
          const inputData = e.inputBuffer.getChannelData(0);
          const resampledData = downsample(inputData, resampleRatio);
          
          // Float32 â†’ Int16 ë³€í™˜
          const pcmData = new Int16Array(resampledData.length);
          for (let i = 0; i < resampledData.length; i++) {
            const s = Math.max(-1, Math.min(1, resampledData[i]));
            pcmData[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
          }
          
          chunkCount++;
          if (chunkCount % 10 === 0) {
            console.log(`[VIDEO-STT] ğŸ“¤ PCM ì²­í¬ ì „ì†¡: #${chunkCount}, ì‹œê°„: ${video.currentTime.toFixed(1)}s`);
          }
          
          ws.send(pcmData.buffer);
        };
        
        // [advice from AI] ë¶„ì„ê¸° ë…¸ë“œ ì—°ê²° (ì†Œë¦¬ ì¶œë ¥ì—ëŠ” ì˜í–¥ ì—†ìŒ)
        source.connect(processor);
        processor.connect(audioContext.destination);
        
        // refì— ì €ì¥ (ì •ë¦¬ìš©)
        (audioContext as any)._processor = processor;
        (audioContext as any)._source = source;

        // [advice from AI] ìº¡ì²˜ ì‹œì‘ ì‹œì ì˜ ë¹„ë””ì˜¤ ì‹œê°„ ì €ì¥ (íƒ€ì„ìŠ¤íƒ¬í”„ ê³„ì‚°ìš©)
        captureStartVideoTimeRef.current = video.currentTime || 0;
        lastLinesCountRef.current = 0;
        setIsCapturing(true);
        updateStatus('capturing');
        console.log(`[VIDEO-STT] ğŸ™ï¸ ìº¡ì²˜ ì‹œì‘! ë¹„ë””ì˜¤ ì‹œê°„: ${captureStartVideoTimeRef.current.toFixed(1)}s`);
      };

      ws.onmessage = (event) => {
        try {
          const data = JSON.parse(event.data);
          
          // [advice from AI] ì„¤ì •/ì¢…ë£Œ ë©”ì‹œì§€ëŠ” ë¬´ì‹œ
          if (data.type === 'config' || data.type === 'ready_to_stop') {
            return;
          }

          const lines = data.lines || [];
          const bufferText = data.buffer_transcription || data.buffer || '';
          const currentVideoTime = videoElementRef.current?.currentTime || 0;

          // ìƒˆë¡œìš´ lines ì²˜ë¦¬ (ìµœì¢… ê²°ê³¼)
          for (let i = lastLinesCountRef.current; i < lines.length; i++) {
            const line = lines[i];
            
            if (!line.text || line.speaker === -2) continue;
            
            const rawText = line.text.trim();
            if (!rawText) continue;
            
            segmentIdRef.current += 1;
            const speaker = line.speaker > 0 ? `í™”ì${line.speaker}` : undefined;
            lastSpeakerRef.current = line.speaker;
            
            const parsedStart = parseTimeString(line.start);
            const parsedEnd = parseTimeString(line.end);
            const captureStartVideoTime = captureStartVideoTimeRef.current;
            
            const startTime = parsedStart !== null 
              ? captureStartVideoTime + parsedStart 
              : currentVideoTime;
            const endTime = parsedEnd !== null 
              ? captureStartVideoTime + parsedEnd 
              : startTime + 3;
            
            const subtitle: VideoAudioSubtitle = {
              id: segmentIdRef.current,
              text: rawText,
              speaker: speaker,
              startTime: startTime,
              endTime: endTime,
              isFinal: true
            };

            // [advice from AI] ìµœì¢… ê²°ê³¼ë§Œ ë¡œê·¸
            console.log(`[STT] ğŸ“ "${rawText.substring(0, 40)}..." [${startTime.toFixed(1)}s~${endTime.toFixed(1)}s]`);
            onSubtitle(subtitle);
          }
          lastLinesCountRef.current = lines.length;

          // ë²„í¼ í…ìŠ¤íŠ¸ (ì‹¤ì‹œê°„ ì¤‘ê°„ ê²°ê³¼) - ë¡œê·¸ ì—†ì´ ì „ë‹¬ë§Œ
          const currentSpeaker = lastSpeakerRef.current;
          const speakerStr = currentSpeaker && currentSpeaker > 0 ? `í™”ì${currentSpeaker}` : undefined;
          
          if (bufferText && bufferText.trim() && onBufferUpdate) {
            onBufferUpdate({
              text: bufferText.trim(),
              speaker: speakerStr
            });
          } else if (onBufferUpdate) {
            // ë¹ˆ ë²„í¼ ì „ë‹¬ (ë¡œê·¸ ì—†ìŒ)
            onBufferUpdate({
              text: '',
              speaker: speakerStr,
              isNoAudio: data.status === 'no_audio_detected'
            });
          }

        } catch (e) {
          console.error('[VIDEO-STT] ë©”ì‹œì§€ íŒŒì‹± ì˜¤ë¥˜:', e);
        }
      };

      ws.onerror = (error) => {
        console.error('[VIDEO-STT] âŒ WebSocket ì˜¤ë¥˜:', error);
        updateStatus('error');
      };

      ws.onclose = () => {
        console.log('[VIDEO-STT] ğŸ”Œ WebSocket ì—°ê²° ì¢…ë£Œ');
        setIsCapturing(false);
        if (status === 'capturing') {
          updateStatus('idle');
        }
      };

    } catch (error) {
      console.error('[VIDEO-STT] âŒ ìº¡ì²˜ ì‹œì‘ ì˜¤ë¥˜:', error);
      updateStatus('error');
      
      if (error instanceof Error) {
        alert(`ì˜¤ë¥˜: ${error.message}`);
      }
    }
  }, [getVideoElement, onSubtitle, onBufferUpdate, updateStatus, wsUrl, status]);

  // ìº¡ì²˜ ì¤‘ì§€
  const stopCaptureRef = useRef<() => void>(() => {});
  
  stopCaptureRef.current = () => {
    if (!wsRef.current && !audioContextRef.current) {
      return;
    }
    
    console.log('[VIDEO-STT] ğŸ›‘ ìº¡ì²˜ ì¤‘ì§€');

    if (wsRef.current && wsRef.current.readyState === WebSocket.OPEN) {
      wsRef.current.send(new Blob([]));
      wsRef.current.close();
    }
    wsRef.current = null;

    if (audioContextRef.current && audioContextRef.current.state !== 'closed') {
      audioContextRef.current.close();
    }
    audioContextRef.current = null;

    setIsCapturing(false);
    updateStatus('idle');
  };
  
  const stopCapture = useCallback(() => {
    stopCaptureRef.current();
  }, []);

  useEffect(() => {
    return () => {
      stopCaptureRef.current();
    };
  }, []);

  return {
    isCapturing,
    status,
    startCapture,
    stopCapture
  };
}
